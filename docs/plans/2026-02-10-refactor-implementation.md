# G3NAT Codebase Refactor Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Refactor G3NAT codebase into clean, well-organized modules with proper separation of concerns, using TDD to ensure no behavior changes.

**Architecture:** Split flat files into organized package structure (models, graph, data, training, utils, visualization). Extract training code from models.py, unify TB and pickle training, elevate sequence_to_graph to dedicated module. All existing functionality preserved, verified by TDD.

**Tech Stack:** Python 3.13, PyTorch, PyTorch Geometric, pytest (for testing)

**Design Document:** See `docs/plans/2026-02-10-refactor-design.md`

---

## Phase 1: Setup & Baseline Tests

### Task 1: Create Package Structure

**Files:**
- Create: `g3nat/__init__.py`
- Create: `g3nat/models/__init__.py`
- Create: `g3nat/graph/__init__.py`
- Create: `g3nat/data/__init__.py`
- Create: `g3nat/training/__init__.py`
- Create: `g3nat/utils/__init__.py`
- Create: `g3nat/visualization/__init__.py`
- Create: `g3nat/evaluation/__init__.py`
- Create: `tests/baseline/__init__.py`

**Step 1: Create directory structure**

```bash
mkdir -p g3nat/models g3nat/graph g3nat/data g3nat/training g3nat/utils g3nat/visualization g3nat/evaluation
mkdir -p tests/baseline tests/test_graph tests/test_models tests/test_data tests/test_training tests/integration
```

**Step 2: Create empty __init__.py files**

```bash
touch g3nat/__init__.py
touch g3nat/models/__init__.py
touch g3nat/graph/__init__.py
touch g3nat/data/__init__.py
touch g3nat/training/__init__.py
touch g3nat/utils/__init__.py
touch g3nat/visualization/__init__.py
touch g3nat/evaluation/__init__.py
touch tests/__init__.py
touch tests/baseline/__init__.py
```

**Step 3: Commit structure**

```bash
git add g3nat/ tests/
git commit -m "Create package structure for refactor

Setting up organized module hierarchy:
- g3nat/ package with submodules
- tests/ directory with baseline tests

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 2: Write Baseline Test for sequence_to_graph

**Files:**
- Create: `tests/baseline/test_baseline_graph.py`

**Step 1: Write baseline capture test**

```python
# tests/baseline/test_baseline_graph.py
import sys
sys.path.insert(0, '.')

import torch
import pickle
from pathlib import Path
from dataset import sequence_to_graph

BASELINE_DIR = Path("tests/baseline/outputs")
BASELINE_DIR.mkdir(exist_ok=True)

def test_capture_sequence_to_graph_simple():
    """Capture current sequence_to_graph behavior for simple sequence."""
    graph = sequence_to_graph(
        primary_sequence="ACGT",
        complementary_sequence="ACGT",
        left_contact_positions=0,
        right_contact_positions=3,
        left_contact_coupling=0.1,
        right_contact_coupling=0.1
    )

    baseline = {
        'x': graph.x.clone(),
        'edge_index': graph.edge_index.clone(),
        'edge_attr': graph.edge_attr.clone(),
        'num_nodes': graph.x.size(0),
        'num_edges': graph.edge_index.size(1)
    }

    with open(BASELINE_DIR / "graph_simple.pkl", "wb") as f:
        pickle.dump(baseline, f)

    print(f"Captured baseline: {graph.x.size(0)} nodes, {graph.edge_index.size(1)} edges")

def test_capture_sequence_to_graph_cross_contacts():
    """Capture behavior with cross contacts (complementary strand)."""
    graph = sequence_to_graph(
        primary_sequence="ACGTACGT",
        complementary_sequence="ACGTACGT",
        left_contact_positions=0,
        right_contact_positions=('complementary', 0),
        left_contact_coupling=0.1,
        right_contact_coupling=0.6
    )

    baseline = {
        'x': graph.x.clone(),
        'edge_index': graph.edge_index.clone(),
        'edge_attr': graph.edge_attr.clone()
    }

    with open(BASELINE_DIR / "graph_cross.pkl", "wb") as f:
        pickle.dump(baseline, f)

if __name__ == "__main__":
    test_capture_sequence_to_graph_simple()
    test_capture_sequence_to_graph_cross_contacts()
    print("Baselines captured successfully")
```

**Step 2: Run baseline capture**

```bash
python tests/baseline/test_baseline_graph.py
```

Expected: Creates `tests/baseline/outputs/graph_simple.pkl` and `graph_cross.pkl`

**Step 3: Commit baseline tests**

```bash
git add tests/baseline/
git commit -m "Add baseline tests for sequence_to_graph

Capture current behavior before refactoring to ensure
no changes in graph construction.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 3: Write Baseline Tests for Models

**Files:**
- Create: `tests/baseline/test_baseline_models.py`

**Step 1: Write baseline capture for model forward pass**

```python
# tests/baseline/test_baseline_models.py
import sys
sys.path.insert(0, '.')

import torch
import pickle
import numpy as np
from pathlib import Path
from models import DNATransportGNN, DNATransportHamiltonianGNN
from dataset import sequence_to_graph

BASELINE_DIR = Path("tests/baseline/outputs")

def test_capture_standard_model():
    """Capture DNATransportGNN predictions."""
    torch.manual_seed(42)

    # Create model
    energy_grid = np.linspace(-3, 3, 100)
    model = DNATransportGNN(
        hidden_dim=64,
        num_layers=2,
        num_heads=2,
        output_dim=100,
        dropout=0.1
    )
    model.eval()

    # Create test graph
    graph = sequence_to_graph("ACGT", "ACGT", 0, 3, 0.1, 0.1)

    # Forward pass
    with torch.no_grad():
        dos_pred, trans_pred = model(graph)

    baseline = {
        'dos': dos_pred.clone(),
        'transmission': trans_pred.clone(),
        'model_state': {k: v.clone() for k, v in model.state_dict().items()}
    }

    with open(BASELINE_DIR / "model_standard.pkl", "wb") as f:
        pickle.dump(baseline, f)

    print(f"Standard model baseline: DOS shape {dos_pred.shape}, Trans shape {trans_pred.shape}")

def test_capture_hamiltonian_model():
    """Capture DNATransportHamiltonianGNN predictions."""
    torch.manual_seed(42)

    energy_grid = np.linspace(-1, 1, 50)
    model = DNATransportHamiltonianGNN(
        hidden_dim=64,
        num_layers=2,
        num_heads=2,
        energy_grid=energy_grid,
        dropout=0.0,
        n_orb=1,
        solver_type='frobenius'
    )
    model.eval()

    graph = sequence_to_graph("ACGT", "ACGT", 0, 3, 0.1, 0.1)

    with torch.no_grad():
        dos_pred, trans_pred = model(graph)

    baseline = {
        'dos': dos_pred.clone(),
        'transmission': trans_pred.clone()
    }

    with open(BASELINE_DIR / "model_hamiltonian.pkl", "wb") as f:
        pickle.dump(baseline, f)

    print(f"Hamiltonian model baseline: DOS shape {dos_pred.shape}")

if __name__ == "__main__":
    test_capture_standard_model()
    test_capture_hamiltonian_model()
    print("Model baselines captured")
```

**Step 2: Run model baseline capture**

```bash
python tests/baseline/test_baseline_models.py
```

Expected: Creates baseline pickle files for both models

**Step 3: Commit model baselines**

```bash
git add tests/baseline/
git commit -m "Add baseline tests for model forward passes

Capture model predictions to verify refactor doesn't
change model behavior.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 4: Write Baseline Tests for Data Loading

**Files:**
- Create: `tests/baseline/test_baseline_data.py`

**Step 1: Write baseline for dataset creation**

```python
# tests/baseline/test_baseline_data.py
import sys
sys.path.insert(0, '.')

import pickle
import numpy as np
from pathlib import Path
from dataset import create_dna_dataset

BASELINE_DIR = Path("tests/baseline/outputs")

def test_capture_dataset_creation():
    """Capture current dataset creation behavior."""
    np.random.seed(42)

    sequences = ["ACGT", "TGCA", "AAAA"]
    complementary = ["ACGT", "TGCA", "TTTT"]
    dos_data = np.random.rand(3, 100)
    trans_data = np.random.rand(3, 100)
    energy_grid = np.linspace(-3, 3, 100)

    dataset = create_dna_dataset(
        sequences=sequences,
        dos_data=dos_data,
        transmission_data=trans_data,
        energy_grid=energy_grid,
        complementary_sequences=complementary
    )

    # Capture first graph
    graph0 = dataset[0]
    baseline = {
        'num_samples': len(dataset),
        'graph0_x': graph0.x.clone(),
        'graph0_edge_index': graph0.edge_index.clone(),
        'graph0_dos': graph0.dos.clone(),
        'graph0_transmission': graph0.transmission.clone()
    }

    with open(BASELINE_DIR / "dataset.pkl", "wb") as f:
        pickle.dump(baseline, f)

    print(f"Dataset baseline: {len(dataset)} samples")

if __name__ == "__main__":
    test_capture_dataset_creation()
    print("Data baseline captured")
```

**Step 2: Run data baseline capture**

```bash
python tests/baseline/test_baseline_data.py
```

**Step 3: Commit data baselines**

```bash
git add tests/baseline/
git commit -m "Add baseline tests for dataset creation

Capture dataset behavior to verify data loading
remains unchanged after refactor.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 2: Core Modules (Dependency Order)

### Task 5: Move Utils Module (No Dependencies)

**Files:**
- Create: `g3nat/utils/device.py`
- Create: `g3nat/utils/physics.py`
- Modify: `g3nat/utils/__init__.py`

**Step 1: Write test for device setup**

```python
# tests/test_utils/test_device.py
import sys
sys.path.insert(0, '.')

import torch

def test_setup_device_auto():
    """Test device setup with auto detection."""
    from g3nat.utils.device import setup_device

    device = setup_device('auto')
    assert isinstance(device, torch.device)
    assert device.type in ['cpu', 'cuda']

def test_setup_device_explicit():
    """Test explicit device selection."""
    from g3nat.utils.device import setup_device

    device = setup_device('cpu')
    assert device.type == 'cpu'
```

**Step 2: Run test (should fail)**

```bash
python -m pytest tests/test_utils/test_device.py -v
```

Expected: ModuleNotFoundError or ImportError

**Step 3: Extract device utilities**

```python
# g3nat/utils/device.py
import torch

def setup_device(device_arg: str) -> torch.device:
    """
    Setup computation device.

    Args:
        device_arg: 'auto', 'cpu', or 'cuda'

    Returns:
        torch.device instance
    """
    if device_arg == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device_arg)

    return device
```

**Step 4: Update utils __init__.py**

```python
# g3nat/utils/__init__.py
from g3nat.utils.device import setup_device

__all__ = ['setup_device']
```

**Step 5: Run test (should pass)**

```bash
python -m pytest tests/test_utils/test_device.py -v
```

Expected: All tests pass

**Step 6: Commit**

```bash
git add g3nat/utils/ tests/test_utils/
git commit -m "Add utils module with device setup

Extract device management from training scripts to
reusable utility module.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 6: Extract Physics Utilities

**Files:**
- Modify: `g3nat/utils/physics.py`
- Modify: `g3nat/utils/__init__.py`

**Step 1: Copy NEGF/Hamiltonian code from data_generator.py**

```python
# g3nat/utils/physics.py
import numpy as np
from typing import Tuple

def create_hamiltonian(seq: str, seq_complementary: str = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Create tight-binding Hamiltonian for DNA sequence.

    Args:
        seq: Primary DNA sequence 5' to 3'
        seq_complementary: Complementary sequence (optional)

    Returns:
        (H_matrix, GammaL, GammaR)
    """
    # Copy exact code from data_generator.py lines 49-152
    if seq_complementary is None:
        seq_complementary = '_' * len(seq)

    assert len(seq) == len(seq_complementary)
    assert '_' not in seq

    onsite_energies = {
        'A': -0.49, 'T': -1.39, 'G': 0.00, 'C': -1.12, '_': 0.00
    }

    HBond_energies = {
        'AA': 0.0, 'CC': 0.0, 'GG': 0.0, 'TT': 0.0,
        'AT': 0.034, 'AG': 0.0, 'AC': 0.0, 'CT': 0.0,
        'CG': 0.050, 'CA': 0.0, 'GT': 0.0, 'GC': 0.050,
        'GA': 0.0, 'TA': 0.034, 'TC': 0.0, 'TG': 0.0
    }

    nn_energies = {
        'AA': 0.030, 'CC': 0.041, 'GG': 0.084, 'TT': 0.158,
        'AT': 0.105, 'AG': 0.049, 'AC': 0.061, 'CT': 0.100,
        'CG': 0.042, 'CA': 0.029, 'GT': 0.137, 'GC': 0.110,
        'GA': 0.089, 'TA': 0.086, 'TC': 0.076, 'TG': 0.085
    }

    full_seq = seq + seq_complementary[::-1]
    H = np.zeros((len(full_seq), len(full_seq)))

    for i in range(len(full_seq)):
        for j in range(len(full_seq)):
            base_1 = full_seq[i]
            base_2 = full_seq[j]
            if i == j:
                H[i, j] = onsite_energies[base_1]
            elif base_1 == '_' or base_2 == '_':
                continue
            elif i % len(seq) == j % len(seq):
                H[i, j] = HBond_energies[base_1 + base_2]
            elif i == j+1 and i//len(seq) == j//len(seq):
                H[i, j] = nn_energies[base_1 + base_2]
            elif i == j-1 and i//len(seq) == j//len(seq):
                H[i, j] = nn_energies[base_2 + base_1]

    ind = 0
    for i in range(len(full_seq)):
        if full_seq[i] == '_':
            H = np.delete(H, ind, axis=0)
            H = np.delete(H, ind, axis=1)
        else:
            ind += 1

    GammaL = np.zeros(ind)
    GammaL[0] = 0.1
    GammaR = np.zeros(ind)
    GammaR[len(seq)-1] = 0.1

    return H, GammaL, GammaR

def calculate_NEGF(H: np.ndarray, GammaL: np.ndarray, GammaR: np.ndarray,
                  energy_grid: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Calculate transmission and DOS using NEGF.

    Args:
        H: Hamiltonian matrix
        GammaL: Left coupling vector
        GammaR: Right coupling vector
        energy_grid: Energy points

    Returns:
        (transmission, DOS)
    """
    # Copy exact code from data_generator.py lines 155-215
    H_size = H.shape[0]
    transmission_data = np.zeros(len(energy_grid))
    dos_data = np.zeros(len(energy_grid))
    delta = 1e-12j

    for n, energy in enumerate(energy_grid):
        SigmaL = -0.5j * np.diag(GammaL)
        SigmaR = -0.5j * np.diag(GammaR)
        sigTot = SigmaL + SigmaR

        A = energy * np.eye(H_size) - H - sigTot + delta
        try:
            Gr = np.linalg.inv(A)
        except np.linalg.LinAlgError:
            Gr = np.linalg.pinv(A + 1e-8j * np.eye(H_size))

        dos_data[n] = -np.trace(np.imag(Gr))/np.pi

        Ga = Gr.conj().T
        GammaL_diag = np.diag(GammaL)
        GammaR_diag = np.diag(GammaR)

        temp1 = np.dot(GammaL_diag, Gr)
        temp2 = np.dot(temp1, GammaR_diag)
        Tcoh = np.dot(temp2, Ga)
        transmission_data[n] = np.real(np.trace(Tcoh))

    transmission_data = np.clip(transmission_data, 1e-16, None)
    dos_data = np.clip(dos_data, 1e-16, None)

    return transmission_data, dos_data
```

**Step 2: Update utils __init__.py**

```python
# g3nat/utils/__init__.py
from g3nat.utils.device import setup_device
from g3nat.utils.physics import create_hamiltonian, calculate_NEGF

__all__ = ['setup_device', 'create_hamiltonian', 'calculate_NEGF']
```

**Step 3: Test physics utilities work**

```bash
python -c "from g3nat.utils import create_hamiltonian, calculate_NEGF; import numpy as np; H, GL, GR = create_hamiltonian('ACGT'); print('Physics utils OK')"
```

Expected: Prints "Physics utils OK"

**Step 4: Commit**

```bash
git add g3nat/utils/
git commit -m "Add physics utilities (NEGF, Hamiltonian)

Extract tight-binding Hamiltonian and NEGF solvers
to reusable utility functions.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 7: Move Graph Module (Depends on Utils)

**Files:**
- Create: `g3nat/graph/construction.py`
- Modify: `g3nat/graph/__init__.py`
- Create: `tests/test_graph/test_construction.py`

**Step 1: Write test comparing to baseline**

```python
# tests/test_graph/test_construction.py
import sys
sys.path.insert(0, '.')

import torch
import pickle
from pathlib import Path

BASELINE_DIR = Path("tests/baseline/outputs")

def test_sequence_to_graph_simple_unchanged():
    """Verify refactored sequence_to_graph produces identical output."""
    from g3nat.graph import sequence_to_graph

    graph = sequence_to_graph(
        primary_sequence="ACGT",
        complementary_sequence="ACGT",
        left_contact_positions=0,
        right_contact_positions=3,
        left_contact_coupling=0.1,
        right_contact_coupling=0.1
    )

    with open(BASELINE_DIR / "graph_simple.pkl", "rb") as f:
        baseline = pickle.load(f)

    assert torch.equal(graph.x, baseline['x'])
    assert torch.equal(graph.edge_index, baseline['edge_index'])
    assert torch.allclose(graph.edge_attr, baseline['edge_attr'])
    assert graph.x.size(0) == baseline['num_nodes']
    assert graph.edge_index.size(1) == baseline['num_edges']

def test_sequence_to_graph_cross_unchanged():
    """Verify cross contacts work identically."""
    from g3nat.graph import sequence_to_graph

    graph = sequence_to_graph(
        primary_sequence="ACGTACGT",
        complementary_sequence="ACGTACGT",
        left_contact_positions=0,
        right_contact_positions=('complementary', 0),
        left_contact_coupling=0.1,
        right_contact_coupling=0.6
    )

    with open(BASELINE_DIR / "graph_cross.pkl", "rb") as f:
        baseline = pickle.load(f)

    assert torch.equal(graph.x, baseline['x'])
    assert torch.equal(graph.edge_index, baseline['edge_index'])
    assert torch.allclose(graph.edge_attr, baseline['edge_attr'])
```

**Step 2: Run test (should fail - module doesn't exist)**

```bash
python -m pytest tests/test_graph/test_construction.py -v
```

Expected: ImportError

**Step 3: Copy sequence_to_graph from dataset.py**

```python
# g3nat/graph/construction.py
import torch
from torch_geometric.data import Data
from typing import Optional, Union, List, Tuple

# Copy BASE_TO_IDX, BASE_FEATURES, CONTACT_FEATURES from dataset.py
BASE_TO_IDX = {'A': 0, 'T': 1, 'G': 2, 'C': 3}
BASE_FEATURES = {
    'A': [1, 0, 0, 0],
    'T': [0, 1, 0, 0],
    'G': [0, 0, 1, 0],
    'C': [0, 0, 0, 1]
}
CONTACT_FEATURES = [0, 0, 0, 0]

def get_edge_features(edge_type: str, directionality: int, coupling: float = 0.0):
    """Return edge features for different edge types."""
    if edge_type == 'backbone':
        return [1, 0, 0, directionality, 0.0]
    elif edge_type == 'hydrogen_bond':
        return [0, 1, 0, directionality, 0.0]
    elif edge_type == 'contact':
        return [0, 0, 1, directionality, coupling]
    else:
        raise ValueError(f"Invalid edge type: {edge_type}")

def sequence_to_graph(primary_sequence: str,
                     complementary_sequence: Optional[str] = None,
                     left_contact_positions: Optional[Union[int, List[int], Tuple[str, Union[int, List[int]]]]] = None,
                     right_contact_positions: Optional[Union[int, List[int], Tuple[str, Union[int, List[int]]]]] = None,
                     left_contact_coupling: Union[float, List[float]] = 0.1,
                     right_contact_coupling: Union[float, List[float]] = 0.1) -> Data:
    """
    Convert DNA sequence to PyTorch Geometric graph.

    Core graph construction method for DNA transport.
    Creates nodes for bases and contacts, edges for backbone,
    hydrogen bonds, and contact connections.

    Args:
        primary_sequence: Main DNA sequence 5' to 3'
        complementary_sequence: Complementary sequence (optional)
        left_contact_positions: Position(s) for left contact
        right_contact_positions: Position(s) for right contact
        left_contact_coupling: Coupling strength(s) for left contact (eV)
        right_contact_coupling: Coupling strength(s) for right contact (eV)

    Returns:
        PyTorch Geometric Data object with graph representation
    """
    # Copy exact implementation from dataset.py lines 35-249
    # [Full implementation here - exact copy]

    if left_contact_positions is None:
        left_contact_positions = 0
    if right_contact_positions is None:
        right_contact_positions = len(primary_sequence) - 1

    def normalize_contact_positions(positions, default_strand='primary'):
        if isinstance(positions, int):
            return ('primary', [positions])
        elif isinstance(positions, list):
            return ('primary', positions)
        elif isinstance(positions, tuple):
            strand, pos_list = positions
            if isinstance(pos_list, int):
                pos_list = [pos_list]
            return (strand, pos_list)
        else:
            raise ValueError(f"Invalid contact positions format: {positions}")

    left_strand, left_positions = normalize_contact_positions(left_contact_positions)
    right_strand, right_positions = normalize_contact_positions(right_contact_positions)

    if isinstance(left_contact_coupling, float):
        left_contact_coupling = [left_contact_coupling] * len(left_positions)
    if isinstance(right_contact_coupling, float):
        right_contact_coupling = [right_contact_coupling] * len(right_positions)

    assert isinstance(left_contact_coupling, list)
    assert isinstance(right_contact_coupling, list)

    if complementary_sequence is None:
        complementary_sequence = '_' * len(primary_sequence)

    node_features = []
    node_to_strand = []
    node_mapping = {}

    node_features.append(CONTACT_FEATURES)
    node_to_strand.append('left_contact')
    node_mapping['left_contact'] = 0

    node_features.append(CONTACT_FEATURES)
    node_to_strand.append('right_contact')
    node_mapping['right_contact'] = 1

    primary_start_idx = 2
    primary_node_count = 0
    for i, base in enumerate(primary_sequence):
        if base != '_':
            node_features.append(BASE_FEATURES[base])
            node_to_strand.append('primary')
            node_mapping[('primary', i)] = primary_start_idx + primary_node_count
            primary_node_count += 1

    complementary_start_idx = primary_start_idx + primary_node_count
    complementary_node_count = 0
    for i, base in enumerate(complementary_sequence):
        if base != '_':
            node_features.append(BASE_FEATURES[base])
            node_to_strand.append('complementary')
            node_mapping[('complementary', i)] = complementary_start_idx + complementary_node_count
            complementary_node_count += 1

    x = torch.tensor(node_features, dtype=torch.float)

    edge_index = []
    edge_attr = []

    for i, pos in enumerate(left_positions):
        if left_strand == 'primary':
            if pos >= len(primary_sequence):
                raise ValueError(f"Left contact position {pos} out of range")
            base_node_idx = node_mapping[('primary', pos)]
        else:
            if pos >= len(complementary_sequence):
                raise ValueError(f"Left contact position {pos} out of range")
            if complementary_sequence[pos] == '_':
                raise ValueError(f"Left contact on blank base")
            base_node_idx = node_mapping[('complementary', pos)]

        coupling = left_contact_coupling[i] if i < len(left_contact_coupling) else left_contact_coupling[-1]

        edge_index.append([node_mapping['left_contact'], base_node_idx])
        edge_attr.append(get_edge_features('contact', 1, coupling))
        edge_index.append([base_node_idx, node_mapping['left_contact']])
        edge_attr.append(get_edge_features('contact', -1, coupling))

    for i, pos in enumerate(right_positions):
        if right_strand == 'primary':
            if pos >= len(primary_sequence):
                raise ValueError(f"Right contact position {pos} out of range")
            base_node_idx = node_mapping[('primary', pos)]
        else:
            if pos >= len(complementary_sequence):
                raise ValueError(f"Right contact position {pos} out of range")
            if complementary_sequence[pos] == '_':
                raise ValueError(f"Right contact on blank base")
            base_node_idx = node_mapping[('complementary', pos)]

        coupling = right_contact_coupling[i] if i < len(right_contact_coupling) else right_contact_coupling[-1]

        edge_index.append([node_mapping['right_contact'], base_node_idx])
        edge_attr.append(get_edge_features('contact', 1, coupling))
        edge_index.append([base_node_idx, node_mapping['right_contact']])
        edge_attr.append(get_edge_features('contact', -1, coupling))

    primary_positions = [pos for pos, base in enumerate(primary_sequence) if base != '_']
    for i in range(len(primary_positions) - 1):
        pos1 = primary_positions[i]
        pos2 = primary_positions[i + 1]

        if pos2 == pos1 + 1:
            node1_idx = node_mapping[('primary', pos1)]
            node2_idx = node_mapping[('primary', pos2)]
            edge_index.append([node1_idx, node2_idx])
            edge_attr.append(get_edge_features('backbone', 1))
            edge_index.append([node2_idx, node1_idx])
            edge_attr.append(get_edge_features('backbone', -1))

    complementary_positions = [pos for pos, base in enumerate(complementary_sequence) if base != '_']
    for i in range(len(complementary_positions) - 1):
        pos1 = complementary_positions[i]
        pos2 = complementary_positions[i + 1]

        if pos2 == pos1 + 1:
            node1_idx = node_mapping[('complementary', pos1)]
            node2_idx = node_mapping[('complementary', pos2)]
            edge_index.append([node1_idx, node2_idx])
            edge_attr.append(get_edge_features('backbone', 1))
            edge_index.append([node2_idx, node1_idx])
            edge_attr.append(get_edge_features('backbone', -1))

    for i in range(len(primary_sequence)):
        primary_base = primary_sequence[i]
        comp_loc = len(primary_sequence) - i - 1
        complementary_base = complementary_sequence[comp_loc] if comp_loc < len(complementary_sequence) else '_'

        if primary_base != '_' and complementary_base != '_':
            primary_idx = node_mapping[('primary', i)]
            complementary_idx = node_mapping[('complementary', comp_loc)]
            edge_index.extend([[primary_idx, complementary_idx], [complementary_idx, primary_idx]])
            edge_attr.extend([get_edge_features('hydrogen_bond', 0), get_edge_features('hydrogen_bond', 0)])

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    edge_attr = torch.tensor(edge_attr, dtype=torch.float)

    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
    try:
        data.num_dna_nodes = int(x.size(0) - 2)
    except Exception:
        pass
    return data
```

**Step 4: Update graph __init__.py**

```python
# g3nat/graph/__init__.py
from g3nat.graph.construction import sequence_to_graph

__all__ = ['sequence_to_graph']
```

**Step 5: Run test (should pass)**

```bash
python -m pytest tests/test_graph/test_construction.py -v
```

Expected: All tests PASS

**Step 6: Commit**

```bash
git add g3nat/graph/ tests/test_graph/
git commit -m "Add graph module with sequence_to_graph

Extract core graph construction to dedicated module.
Tests verify identical behavior to original.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 8: Move Models Module

**Files:**
- Create: `g3nat/models/standard.py`
- Create: `g3nat/models/hamiltonian.py`
- Modify: `g3nat/models/__init__.py`
- Create: `tests/test_models/test_forward.py`

**Step 1: Write test for models**

```python
# tests/test_models/test_forward.py
import sys
sys.path.insert(0, '.')

import torch
import pickle
import numpy as np
from pathlib import Path

BASELINE_DIR = Path("tests/baseline/outputs")

def test_standard_model_unchanged():
    """Verify DNATransportGNN produces identical predictions."""
    torch.manual_seed(42)

    from g3nat.models import DNATransportGNN
    from g3nat.graph import sequence_to_graph

    model = DNATransportGNN(
        hidden_dim=64,
        num_layers=2,
        num_heads=2,
        output_dim=100,
        dropout=0.1
    )

    with open(BASELINE_DIR / "model_standard.pkl", "rb") as f:
        baseline = pickle.load(f)

    model.load_state_dict(baseline['model_state'])
    model.eval()

    graph = sequence_to_graph("ACGT", "ACGT", 0, 3, 0.1, 0.1)

    with torch.no_grad():
        dos_pred, trans_pred = model(graph)

    assert torch.allclose(dos_pred, baseline['dos'], atol=1e-5)
    assert torch.allclose(trans_pred, baseline['transmission'], atol=1e-5)

def test_hamiltonian_model_unchanged():
    """Verify DNATransportHamiltonianGNN produces identical predictions."""
    torch.manual_seed(42)

    from g3nat.models import DNATransportHamiltonianGNN
    from g3nat.graph import sequence_to_graph

    energy_grid = np.linspace(-1, 1, 50)
    model = DNATransportHamiltonianGNN(
        hidden_dim=64,
        num_layers=2,
        num_heads=2,
        energy_grid=energy_grid,
        dropout=0.0,
        n_orb=1,
        solver_type='frobenius'
    )
    model.eval()

    graph = sequence_to_graph("ACGT", "ACGT", 0, 3, 0.1, 0.1)

    with torch.no_grad():
        dos_pred, trans_pred = model(graph)

    with open(BASELINE_DIR / "model_hamiltonian.pkl", "rb") as f:
        baseline = pickle.load(f)

    # Initialization is random, so we just check shapes and types
    assert dos_pred.shape == baseline['dos'].shape
    assert trans_pred.shape == baseline['transmission'].shape
    assert torch.isfinite(dos_pred).all()
    assert torch.isfinite(trans_pred).all()
```

**Step 2: Run test (should fail)**

```bash
python -m pytest tests/test_models/test_forward.py -v
```

**Step 3: Copy DNATransportGNN to standard.py**

```python
# g3nat/models/standard.py
# Copy DNATransportGNN class from models.py lines 16-112
# (Exact copy - no changes)
```

**Step 4: Copy DNATransportHamiltonianGNN to hamiltonian.py**

```python
# g3nat/models/hamiltonian.py
# Copy DNATransportHamiltonianGNN class from models.py lines 116-720
# (Exact copy - includes NEGF methods)
```

**Step 5: Update models __init__.py**

```python
# g3nat/models/__init__.py
from g3nat.models.standard import DNATransportGNN
from g3nat.models.hamiltonian import DNATransportHamiltonianGNN

__all__ = ['DNATransportGNN', 'DNATransportHamiltonianGNN']
```

**Step 6: Run test (should pass)**

```bash
python -m pytest tests/test_models/test_forward.py -v
```

**Step 7: Commit**

```bash
git add g3nat/models/ tests/test_models/
git commit -m "Add models module (standard and hamiltonian)

Extract model definitions from monolithic models.py.
Training code will move to separate training module.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 9: Move Data Module

**Files:**
- Create: `g3nat/data/datasets.py`
- Create: `g3nat/data/synthetic.py`
- Create: `g3nat/data/pickle.py`
- Modify: `g3nat/data/__init__.py`
- Create: `tests/test_data/test_datasets.py`

**Step 1: Write test for dataset**

```python
# tests/test_data/test_datasets.py
import sys
sys.path.insert(0, '.')

import torch
import pickle
import numpy as np
from pathlib import Path

BASELINE_DIR = Path("tests/baseline/outputs")

def test_dataset_creation_unchanged():
    """Verify create_dna_dataset produces identical results."""
    np.random.seed(42)

    from g3nat.data import DNATransportDataset, create_dna_dataset

    sequences = ["ACGT", "TGCA", "AAAA"]
    complementary = ["ACGT", "TGCA", "TTTT"]
    dos_data = np.random.rand(3, 100)
    trans_data = np.random.rand(3, 100)
    energy_grid = np.linspace(-3, 3, 100)

    dataset = create_dna_dataset(
        sequences=sequences,
        dos_data=dos_data,
        transmission_data=trans_data,
        energy_grid=energy_grid,
        complementary_sequences=complementary
    )

    with open(BASELINE_DIR / "dataset.pkl", "rb") as f:
        baseline = pickle.load(f)

    assert len(dataset) == baseline['num_samples']

    graph0 = dataset[0]
    assert torch.equal(graph0.x, baseline['graph0_x'])
    assert torch.equal(graph0.edge_index, baseline['graph0_edge_index'])
    assert torch.allclose(graph0.dos, baseline['graph0_dos'])
    assert torch.allclose(graph0.transmission, baseline['graph0_transmission'])
```

**Step 2: Run test (should fail)**

```bash
python -m pytest tests/test_data/test_datasets.py -v
```

**Step 3: Copy dataset code**

```python
# g3nat/data/datasets.py
# Copy DNATransportDataset and create_dna_dataset from dataset.py
# (Lines 252-442)
```

**Step 4: Copy synthetic data generation**

```python
# g3nat/data/synthetic.py
# Copy from data_generator.py:
# - create_sample_data (renamed to generate_tight_binding_data)
# - Helper functions
```

**Step 5: Copy pickle loading**

```python
# g3nat/data/pickle.py
# Copy entire load_pickle_data.py (no changes)
```

**Step 6: Update data __init__.py**

```python
# g3nat/data/__init__.py
from g3nat.data.datasets import DNATransportDataset, create_dna_dataset
from g3nat.data.synthetic import generate_tight_binding_data
from g3nat.data.pickle import load_pickle_directory, load_single_pickle

__all__ = [
    'DNATransportDataset',
    'create_dna_dataset',
    'generate_tight_binding_data',
    'load_pickle_directory',
    'load_single_pickle'
]
```

**Step 7: Run test (should pass)**

```bash
python -m pytest tests/test_data/test_datasets.py -v
```

**Step 8: Commit**

```bash
git add g3nat/data/ tests/test_data/
git commit -m "Add data module (datasets, synthetic, pickle)

Organize data loading: synthetic TB data, pickle files,
and PyTorch dataset classes.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 10: Move Training Module

**Files:**
- Create: `g3nat/training/trainer.py`
- Create: `g3nat/training/config.py`
- Create: `g3nat/training/callbacks.py`
- Create: `g3nat/training/utils.py`
- Modify: `g3nat/training/__init__.py`

**Step 1: Extract training config**

```python
# g3nat/training/config.py
from dataclasses import dataclass
from typing import Optional

@dataclass
class TrainingConfig:
    """Configuration for training DNA transport models."""
    num_epochs: int = 100
    learning_rate: float = 1e-3
    batch_size: int = 32
    device: str = 'auto'
    max_grad_norm: float = 1.0
    checkpoint_frequency: int = 10
    checkpoint_dir: Optional[str] = None

    @classmethod
    def from_kwargs(cls, **kwargs):
        """Create config from keyword arguments."""
        valid_fields = {f.name for f in cls.__dataclass_fields__.values()}
        filtered = {k: v for k, v in kwargs.items() if k in valid_fields}
        return cls(**filtered)
```

**Step 2: Extract callbacks**

```python
# g3nat/training/callbacks.py
import os
import json
import time
import torch
from typing import Dict, List

def save_checkpoint(model, optimizer, epoch, train_losses, val_losses,
                   args, energy_grid, checkpoint_path):
    """Save training checkpoint."""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'args': args,
        'energy_grid': energy_grid,
        'timestamp': time.time()
    }
    torch.save(checkpoint, checkpoint_path)

def save_progress_file(epoch, train_loss, val_loss, checkpoint_dir, args):
    """Save lightweight progress tracking."""
    progress_file = os.path.join(checkpoint_dir, 'training_progress.json')
    progress_data = {
        'epoch': epoch,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'timestamp': time.time(),
        'args': args
    }
    with open(progress_file, 'w') as f:
        json.dump(progress_data, f, indent=2)
```

**Step 3: Extract training utilities**

```python
# g3nat/training/utils.py
# Copy from train_from_TB.py:
# - LengthBucketBatchSampler
# - split_dataset
```

**Step 4: Create Trainer class**

```python
# g3nat/training/trainer.py
import torch
import torch.nn as nn
from typing import Optional, Callable, List, Tuple
from torch_geometric.loader import DataLoader

class Trainer:
    """Unified trainer for DNA transport models."""

    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config

    def fit(self, checkpoint_callback=None, progress_callback=None,
            start_epoch=0, train_losses=None, val_losses=None,
            optimizer=None) -> Tuple[List[float], List[float]]:
        """
        Run training loop.

        Returns:
            (train_losses, val_losses)
        """
        # Copy train_model logic from models.py
        # This is the main training loop
        if train_losses is None:
            train_losses = []
        if val_losses is None:
            val_losses = []

        device = torch.device(self.config.device if self.config.device != 'auto'
                            else ('cuda' if torch.cuda.is_available() else 'cpu'))

        self.model = self.model.to(device)

        if optimizer is None:
            optimizer = torch.optim.Adam(self.model.parameters(),
                                        lr=self.config.learning_rate)

        criterion = nn.HuberLoss()

        for epoch in range(start_epoch, self.config.num_epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0
            for batch in self.train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()

                dos_pred, trans_pred = self.model(batch)

                batch_size = dos_pred.size(0)
                num_energy = dos_pred.size(1)

                dos_target = batch.dos.view(batch_size, num_energy)
                trans_target = batch.transmission.view(batch_size, num_energy)

                loss = criterion(dos_pred, dos_target) + criterion(trans_pred, trans_target)
                loss.backward()

                if self.config.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),
                                                  self.config.max_grad_norm)

                optimizer.step()
                train_loss += loss.item()

            train_loss /= len(self.train_loader)
            train_losses.append(train_loss)

            # Validation phase
            self.model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch in self.val_loader:
                    batch = batch.to(device)
                    dos_pred, trans_pred = self.model(batch)

                    batch_size = dos_pred.size(0)
                    num_energy = dos_pred.size(1)
                    dos_target = batch.dos.view(batch_size, num_energy)
                    trans_target = batch.transmission.view(batch_size, num_energy)

                    loss = criterion(dos_pred, dos_target) + criterion(trans_pred, trans_target)
                    val_loss += loss.item()

            val_loss /= len(self.val_loader) if len(self.val_loader) > 0 else 1
            val_losses.append(val_loss)

            if epoch % 10 == 0:
                print(f"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")

            if checkpoint_callback and epoch % self.config.checkpoint_frequency == 0:
                checkpoint_callback(self.model, optimizer, epoch, train_losses, val_losses)

            if progress_callback:
                progress_callback(epoch, train_loss, val_loss)

        return train_losses, val_losses

def train_model(model, train_loader, val_loader, **kwargs):
    """
    Convenience function for training (backward compatible).

    Wraps Trainer class with simpler interface.
    """
    from g3nat.training.config import TrainingConfig

    config = TrainingConfig.from_kwargs(**kwargs)
    trainer = Trainer(model, train_loader, val_loader, config)

    return trainer.fit(
        checkpoint_callback=kwargs.get('checkpoint_callback'),
        progress_callback=kwargs.get('progress_callback'),
        start_epoch=kwargs.get('start_epoch', 0),
        train_losses=kwargs.get('train_losses'),
        val_losses=kwargs.get('val_losses'),
        optimizer=kwargs.get('optimizer')
    )
```

**Step 5: Update training __init__.py**

```python
# g3nat/training/__init__.py
from g3nat.training.trainer import Trainer, train_model
from g3nat.training.config import TrainingConfig
from g3nat.training.callbacks import save_checkpoint, save_progress_file
from g3nat.training.utils import LengthBucketBatchSampler, split_dataset

__all__ = [
    'Trainer',
    'train_model',
    'TrainingConfig',
    'save_checkpoint',
    'save_progress_file',
    'LengthBucketBatchSampler',
    'split_dataset'
]
```

**Step 6: Test training module works**

```bash
python -c "from g3nat.training import train_model, Trainer, TrainingConfig; print('Training module OK')"
```

**Step 7: Commit**

```bash
git add g3nat/training/
git commit -m "Add training module (extracted from models.py)

Separate training logic from model definitions.
Provides Trainer class and backward-compatible
train_model function.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 11: Move Visualization Module

**Files:**
- Create: `g3nat/visualization/graphs.py`
- Modify: `g3nat/visualization/__init__.py`

**Step 1: Copy visualization code**

```python
# g3nat/visualization/graphs.py
# Copy entire visualize_dna_graph.py (no changes to logic)
```

**Step 2: Update visualization __init__.py**

```python
# g3nat/visualization/__init__.py
from g3nat.visualization.graphs import visualize_dna_graph

__all__ = ['visualize_dna_graph']
```

**Step 3: Test import**

```bash
python -c "from g3nat.visualization import visualize_dna_graph; print('Visualization OK')"
```

**Step 4: Commit**

```bash
git add g3nat/visualization/
git commit -m "Add visualization module

Move DNA graph visualization to dedicated module.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 3: Scripts & Package Setup

### Task 12: Update Package-Level Imports

**Files:**
- Modify: `g3nat/__init__.py`

**Step 1: Create clean package API**

```python
# g3nat/__init__.py
"""
G3NAT: Graph Neural Network for DNA Transport Properties

A Python package for predicting electronic transport properties
of DNA using graph neural networks and tight-binding methods.
"""

__version__ = "0.2.0"

from g3nat.models import DNATransportGNN, DNATransportHamiltonianGNN
from g3nat.graph import sequence_to_graph
from g3nat.training import train_model, Trainer
from g3nat.data import DNATransportDataset, create_dna_dataset

__all__ = [
    "DNATransportGNN",
    "DNATransportHamiltonianGNN",
    "sequence_to_graph",
    "train_model",
    "Trainer",
    "DNATransportDataset",
    "create_dna_dataset",
]
```

**Step 2: Test clean imports**

```bash
python -c "import g3nat; print(f'G3NAT v{g3nat.__version__}'); print('Available:', ', '.join(g3nat.__all__))"
```

Expected: Prints version and available classes

**Step 3: Commit**

```bash
git add g3nat/__init__.py
git commit -m "Add package-level API and version

Expose clean API at package level: import g3nat
Version 0.2.0 reflects refactor.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 13: Create Unified Training Script

**Files:**
- Create: `scripts/train.py`

**Step 1: Create unified training script**

```python
# scripts/train.py
#!/usr/bin/env python3
"""Unified training script for G3NAT (TB and pickle data)."""

import argparse
import os
import sys

# Ensure g3nat package is importable
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np

import g3nat
from g3nat.data import (generate_tight_binding_data, load_pickle_directory,
                        create_dna_dataset)
from g3nat.training import train_model, TrainingConfig, LengthBucketBatchSampler
from g3nat.training.callbacks import save_checkpoint, save_progress_file
from g3nat.utils import setup_device

from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
from torch.utils.data import Subset

def parse_args():
    parser = argparse.ArgumentParser(description='Train DNA Transport GNN')

    parser.add_argument('--data_source', type=str, required=True,
                       choices=['tb', 'pickle'],
                       help='Data source: tb (tight-binding) or pickle')

    # Data parameters
    parser.add_argument('--data_dir', type=str, default=None,
                       help='Directory with pickle files (for pickle source)')
    parser.add_argument('--num_samples', type=int, default=2000,
                       help='Number of samples (for TB source)')
    parser.add_argument('--seq_length', type=int, default=8,
                       help='Sequence length (for TB source)')
    parser.add_argument('--num_energy_points', type=int, default=100,
                       help='Number of energy points')

    # Model parameters
    parser.add_argument('--model_type', type=str, default='hamiltonian',
                       choices=['standard', 'hamiltonian'])
    parser.add_argument('--hidden_dim', type=int, default=128)
    parser.add_argument('--num_layers', type=int, default=4)
    parser.add_argument('--num_heads', type=int, default=4)
    parser.add_argument('--dropout', type=float, default=0.0)
    parser.add_argument('--n_orb', type=int, default=1)

    # Training parameters
    parser.add_argument('--num_epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--learning_rate', type=float, default=1e-3)
    parser.add_argument('--device', type=str, default='auto')

    # Output parameters
    parser.add_argument('--output_dir', type=str, default='./outputs')
    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints')

    return parser.parse_args()

def main():
    args = parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.checkpoint_dir, exist_ok=True)

    print(f"G3NAT Training (v{g3nat.__version__})")
    print(f"Data source: {args.data_source}")
    print(f"Model type: {args.model_type}")

    device = setup_device(args.device)
    print(f"Device: {device}")

    # Load data
    if args.data_source == 'tb':
        print(f"Generating {args.num_samples} TB samples...")
        seqs, comp_seqs, dos_data, trans_data, energy_grid = generate_tight_binding_data(
            num_samples=args.num_samples,
            seq_length=args.seq_length,
            num_energy_points=args.num_energy_points
        )
    else:  # pickle
        if args.data_dir is None:
            raise ValueError("--data_dir required for pickle data source")
        print(f"Loading pickle files from {args.data_dir}...")
        seqs, comp_seqs, dos_data, trans_data, energy_grid, contact_configs = \
            load_pickle_directory(args.data_dir)

        # Extract contact configurations for pickle data
        left_contact_pos_list = [c['left_contact_pos'] for c in contact_configs]
        right_contact_pos_list = [c['right_contact_pos'] for c in contact_configs]
        left_coupling_list = [c['coupling'] for c in contact_configs]
        right_coupling_list = [c['coupling'] for c in contact_configs]

    print(f"Loaded {len(seqs)} samples")

    # Create dataset
    if args.data_source == 'pickle':
        dataset = create_dna_dataset(
            sequences=seqs,
            dos_data=dos_data,
            transmission_data=trans_data,
            energy_grid=energy_grid,
            complementary_sequences=comp_seqs,
            left_contact_positions_list=left_contact_pos_list,
            right_contact_positions_list=right_contact_pos_list,
            left_contact_coupling_list=left_coupling_list,
            right_contact_coupling_list=right_coupling_list
        )
    else:
        dataset = create_dna_dataset(
            sequences=seqs,
            dos_data=dos_data,
            transmission_data=trans_data,
            energy_grid=energy_grid,
            complementary_sequences=comp_seqs
        )

    # Split dataset
    train_indices, val_indices = train_test_split(
        range(len(dataset)), test_size=0.2, random_state=42
    )
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)

    # Create loaders
    is_hamiltonian = (args.model_type == 'hamiltonian')
    if is_hamiltonian:
        train_sampler = LengthBucketBatchSampler(train_dataset, args.batch_size, shuffle=True)
        val_sampler = LengthBucketBatchSampler(val_dataset, args.batch_size, shuffle=False)
        train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)
        val_loader = DataLoader(val_dataset, batch_sampler=val_sampler)
    else:
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)

    # Create model
    if args.model_type == 'standard':
        model = g3nat.DNATransportGNN(
            hidden_dim=args.hidden_dim,
            num_layers=args.num_layers,
            num_heads=args.num_heads,
            output_dim=len(energy_grid),
            dropout=args.dropout
        )
    else:
        model = g3nat.DNATransportHamiltonianGNN(
            hidden_dim=args.hidden_dim,
            num_layers=args.num_layers,
            num_heads=args.num_heads,
            energy_grid=energy_grid,
            dropout=args.dropout,
            n_orb=args.n_orb
        )

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Train
    def checkpoint_cb(model, opt, epoch, train_losses, val_losses):
        save_checkpoint(model, opt, epoch, train_losses, val_losses,
                       vars(args), energy_grid,
                       os.path.join(args.checkpoint_dir, 'checkpoint_latest.pth'))

    def progress_cb(epoch, train_loss, val_loss):
        save_progress_file(epoch, train_loss, val_loss, args.checkpoint_dir, vars(args))

    print("Training...")
    train_losses, val_losses = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        device=str(device),
        checkpoint_frequency=10,
        checkpoint_callback=checkpoint_cb,
        progress_callback=progress_cb
    )

    # Save final model
    model_path = os.path.join(args.output_dir, f'{args.model_type}_{args.data_source}_model.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'args': vars(args),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'energy_grid': energy_grid
    }, model_path)

    print(f"Training complete!")
    print(f"Model saved: {model_path}")
    print(f"Final train loss: {train_losses[-1]:.4f}")
    print(f"Final val loss: {val_losses[-1]:.4f}")

if __name__ == '__main__':
    main()
```

**Step 2: Test script works**

```bash
python scripts/train.py --data_source tb --num_samples 10 --num_epochs 2 --batch_size 4
```

Expected: Runs training for 2 epochs

**Step 3: Commit**

```bash
git add scripts/train.py
git commit -m "Add unified training script

Single script for both TB and pickle data sources.
Replaces train_from_TB.py and train_from_pickles.py.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 14: Update Ablation Script

**Files:**
- Modify: `ablate.py`  `scripts/ablate.py`

**Step 1: Update imports in ablate.py**

```python
# scripts/ablate.py
# Change imports from:
# from models import DNATransportGNN, DNATransportHamiltonianGNN, train_model
# from data_generator import create_sample_data
# from dataset import create_dna_dataset

# To:
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import g3nat
from g3nat.models import DNATransportGNN, DNATransportHamiltonianGNN
from g3nat.training import train_model
from g3nat.data import generate_tight_binding_data, create_dna_dataset
```

**Step 2: Update function calls**

Replace `create_sample_data` with `generate_tight_binding_data` throughout

**Step 3: Test ablation script**

```bash
python scripts/ablate.py --out outputs/test_ablate
```

**Step 4: Commit**

```bash
git add scripts/ablate.py
git commit -m "Update ablation script for new module structure

Update imports to use g3nat package instead of
flat file imports.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 4: Integration Tests & Cleanup

### Task 15: Write End-to-End Integration Test

**Files:**
- Create: `tests/integration/test_end_to_end.py`

**Step 1: Write full pipeline test**

```python
# tests/integration/test_end_to_end.py
import sys
sys.path.insert(0, '.')

import torch
import numpy as np

def test_full_training_pipeline_tb():
    """Test complete training pipeline with TB data."""
    import g3nat
    from g3nat.data import generate_tight_binding_data, create_dna_dataset
    from g3nat.training import train_model
    from torch_geometric.loader import DataLoader
    from sklearn.model_selection import train_test_split
    from torch.utils.data import Subset

    torch.manual_seed(42)
    np.random.seed(42)

    # Generate data
    seqs, comp_seqs, dos_data, trans_data, energy_grid = generate_tight_binding_data(
        num_samples=20,
        seq_length=6,
        num_energy_points=50
    )

    # Create dataset
    dataset = create_dna_dataset(
        sequences=seqs,
        dos_data=dos_data,
        transmission_data=trans_data,
        energy_grid=energy_grid,
        complementary_sequences=comp_seqs
    )

    # Split
    train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)
    train_ds = Subset(dataset, train_idx)
    val_ds = Subset(dataset, val_idx)

    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)

    # Create model
    model = g3nat.DNATransportGNN(
        hidden_dim=32,
        num_layers=2,
        num_heads=2,
        output_dim=50,
        dropout=0.1
    )

    # Train
    train_losses, val_losses = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=2,
        learning_rate=1e-3,
        device='cpu'
    )

    assert len(train_losses) == 2
    assert len(val_losses) == 2
    assert all(torch.isfinite(torch.tensor(l)) for l in train_losses)
    assert all(torch.isfinite(torch.tensor(l)) for l in val_losses)

    print(f"Integration test passed: train_loss={train_losses[-1]:.4f}, val_loss={val_losses[-1]:.4f}")

if __name__ == "__main__":
    test_full_training_pipeline_tb()
    print("End-to-end integration test PASSED")
```

**Step 2: Run integration test**

```bash
python tests/integration/test_end_to_end.py
```

Expected: Test passes, prints losses

**Step 3: Commit**

```bash
git add tests/integration/
git commit -m "Add end-to-end integration test

Test complete training pipeline from data generation
through training to verify refactor didn't break workflow.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 16: Run All Tests

**Step 1: Run complete test suite**

```bash
python -m pytest tests/ -v
```

Expected: All tests pass

**Step 2: If any tests fail, debug and fix**

(Use systematic debugging if needed)

**Step 3: Document test results**

```bash
python -m pytest tests/ -v > test_results.txt
git add test_results.txt
git commit -m "Add test results for refactor verification

All tests pass, confirming behavior is unchanged.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 17: Clean Up Old Files

**Step 1: Remove old files (after confirming tests pass)**

```bash
# Keep old files as .old for one commit (safety)
mv models.py models.py.old
mv dataset.py dataset.py.old
mv data_generator.py data_generator.py.old
mv load_pickle_data.py load_pickle_data.py.old
mv train_from_TB.py train_from_TB.py.old
mv train_from_pickles.py train_from_pickles.py.old
mv visualize_dna_graph.py visualize_dna_graph.py.old
```

**Step 2: Commit old files as backup**

```bash
git add *.old
git commit -m "Backup old files before deletion

Keeping .old versions for one commit as safety measure.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

**Step 3: Remove .old files**

```bash
rm *.old
git add -A
git commit -m "Remove old refactored files

Refactor complete. Old code replaced by clean module structure.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

### Task 18: Update Documentation

**Files:**
- Create: `README_REFACTOR.md`

**Step 1: Document new structure**

```markdown
# G3NAT Module Structure (Post-Refactor)

## Package Organization

```
g3nat/
 models/          # Model architectures (GNN, Hamiltonian)
 graph/           # Graph construction (sequence_to_graph)
 data/            # Data loading (synthetic, pickle, datasets)
 training/        # Training loop, config, callbacks
 utils/           # Device management, physics utilities
 visualization/   # Plotting and visualization
 evaluation/      # Evaluation metrics
```

## Quick Start

```python
import g3nat

# Create model
model = g3nat.DNATransportHamiltonianGNN(...)

# Create graph
graph = g3nat.sequence_to_graph("ACGT", ...)

# Train
results = g3nat.train_model(model, train_loader, val_loader)
```

## Training Scripts

**Unified training:** `scripts/train.py`
- Supports both TB and pickle data
- Single interface for all training

**Ablation studies:** `scripts/ablate.py`

## Migration Guide

Old import  New import:
- `from models import DNATransportGNN`  `from g3nat.models import DNATransportGNN`
- `from dataset import sequence_to_graph`  `from g3nat.graph import sequence_to_graph`
- `from models import train_model`  `from g3nat.training import train_model`

## Testing

Run test suite:
```bash
python -m pytest tests/ -v
```

All tests verify refactored code produces identical behavior to original.
```

**Step 2: Commit documentation**

```bash
git add README_REFACTOR.md
git commit -m "Add refactor documentation

Document new module structure and migration guide.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Implementation Notes

**TDD Workflow:**
1. Write baseline tests FIRST (capture current behavior)
2. For each module refactor:
   - Write test comparing to baseline
   - Move code to new location
   - Run test to verify identical behavior
   - Commit

**Dependency Order:**
- utils  graph  models  data  training
- Must follow this order to avoid import errors

**Testing Philosophy:**
- Baseline tests capture ground truth
- Refactor tests verify no behavior changes
- Integration tests verify full pipeline works

**Commit Strategy:**
- Frequent small commits (one module at a time)
- Each commit should pass all tests
- Clear messages explaining what moved where

**Success Criteria:**
- All baseline tests match refactored code
- Full training pipeline runs end-to-end
- Clean module structure with no circular dependencies
- Scripts work with new imports
